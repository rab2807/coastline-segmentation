{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9748880,"sourceType":"datasetVersion","datasetId":5968422}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom PIL import Image\nimport tifffile\nimport matplotlib.pyplot as plt\nimport cv2\n\nimport random","metadata":{"execution":{"iopub.status.busy":"2024-12-14T14:29:51.171728Z","iopub.execute_input":"2024-12-14T14:29:51.171975Z","iopub.status.idle":"2024-12-14T14:29:53.463124Z","shell.execute_reply.started":"2024-12-14T14:29:51.171946Z","shell.execute_reply":"2024-12-14T14:29:53.462204Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"DEFAULT_RANDOM_SEED = 2021\n\nseed = DEFAULT_RANDOM_SEED\n\nrandom.seed(seed)\n# os.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\n\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)","metadata":{"execution":{"iopub.status.busy":"2024-12-14T14:29:53.465281Z","iopub.execute_input":"2024-12-14T14:29:53.466088Z","iopub.status.idle":"2024-12-14T14:29:53.473810Z","shell.execute_reply.started":"2024-12-14T14:29:53.466042Z","shell.execute_reply":"2024-12-14T14:29:53.473099Z"},"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"'''DATA LOADER'''\nclass SWEDDataset(Dataset):\n    def __init__(self, root_dir, mode='train', transform=None, target_transform=None, index=True):\n        self.root_dir = root_dir\n        self.mode = mode\n        self.transform = transform\n        self.target_transform = target_transform\n        self.index = index\n\n        self.data_dir = os.path.join(root_dir, mode)\n        self.image_dir = os.path.join(self.data_dir, 'images')\n        self.label_dir = os.path.join(self.data_dir, 'labels')\n        \n        image_files = sorted([f for f in os.listdir(self.image_dir) \n                            if f.endswith('.npy' if mode in ['train', 'val'] else '.tif')])\n        label_files = sorted([f for f in os.listdir(self.label_dir) \n                            if f.endswith('.npy' if mode in ['train', 'val'] else '.tif')])\n        \n        self.pairs = []\n        label_suffix = '_chip_' if mode in ['train', 'val'] else '_label_'\n        image_dict = {f.replace('_image_', label_suffix): f for f in image_files}\n        \n        for label_file in label_files:\n            if label_file in image_dict:\n                self.pairs.append((image_dict[label_file], label_file))\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        img_file, label_file = self.pairs[idx]\n        img_path = os.path.join(self.image_dir, img_file)\n        label_path = os.path.join(self.label_dir, label_file)\n        \n        if self.mode in ['train', 'val']:\n            image = np.load(img_path)\n            label = np.load(label_path)\n        else:\n            image = tifffile.imread(img_path)\n            label = tifffile.imread(label_path)\n            \n        image = torch.from_numpy(image).float()\n        label = torch.from_numpy(label).float()\n        \n        if self.mode in ['train', 'val']:\n            image = image.permute(2, 0, 1)\n        elif self.mode == 'test':\n            image = image.permute(0, 2, 1)\n            label = label.unsqueeze(0)\n            label = torch.rot90(label, 1, [1, 2])\n            label = torch.flip(label, [1])\n\n        image = image / 2.0**15     # jp2 images are 8 to 16 bit\n        label = label > 0.0         # binary label\n\n        # bands: B1, B2, B3, B4, B5, B6, B7, B8, B8A, B9, B11, B12\n        if self.index:\n            ndwi1 = (image[2] - image[7]) / (image[2] + image[7] + 1e-6)\n            \n            # downsample\n            lowres = image[2].T.cpu().numpy()\n            original_shape = lowres.shape\n            lowres = cv2.resize(\n                lowres, \n                (lowres.shape[1] // 2, lowres.shape[0] // 2), \n                interpolation=cv2.INTER_CUBIC\n            )\n            lowres = cv2.resize(\n                lowres, \n                (original_shape[1], original_shape[0]), \n                interpolation=cv2.INTER_CUBIC\n            )\n            lowres = torch.from_numpy(lowres.T).float()\n\n            ndwi2 = (lowres - image[10]) / (lowres + image[10] + 1e-6)\n\n            image = torch.cat([image, ndwi1.unsqueeze(0), ndwi2.unsqueeze(0)], dim=0)\n            \n        if self.transform:\n            image = self.transform(image)\n            label = self.transform(label)\n\n        if self.target_transform:\n            label = self.target_transform(label)\n            \n        return image, label","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-12-14T14:29:53.474908Z","iopub.execute_input":"2024-12-14T14:29:53.475325Z","iopub.status.idle":"2024-12-14T14:29:53.491504Z","shell.execute_reply.started":"2024-12-14T14:29:53.475241Z","shell.execute_reply":"2024-12-14T14:29:53.490806Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"'''GET DATA LOADER'''\ndef get_dataloaders(root_dir, batch_size=32, num_workers=4, train_transform=None, test_transform=None, device='cuda'):\n    train_dataset = SWEDDataset(root_dir, mode='train', transform=train_transform)\n    test_dataset = SWEDDataset(root_dir, mode='test', transform=test_transform)\n\n    # train_dataset, val_dataset, _ = random_split(train_dataset, [round(0.04 * len(train_dataset)), \n    #                                                              round(0.04 * len(train_dataset)), \n    #                                                              len(train_dataset) - round(0.08 * len(train_dataset))])\n\n    train_dataset, val_dataset = random_split(train_dataset, [int(0.8 * len(train_dataset)),  \n                                                                 len(train_dataset) - int(0.8 * len(train_dataset))])\n\n    print(f'Train size: {len(train_dataset)}')\n    print(f'Validation size: {len(val_dataset)}')\n    print(f'Test size: {len(test_dataset)}')\n    \n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True\n    )\n\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=True\n    )\n    \n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=True\n    )\n    \n    return train_loader, val_loader, test_loader","metadata":{"execution":{"iopub.status.busy":"2024-12-14T14:29:53.492488Z","iopub.execute_input":"2024-12-14T14:29:53.492709Z","iopub.status.idle":"2024-12-14T14:29:53.508916Z","shell.execute_reply.started":"2024-12-14T14:29:53.492686Z","shell.execute_reply":"2024-12-14T14:29:53.508157Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"'''DISPLAY SAMPLES'''\ndef display_samples(dataloader, num_samples=5, index=True):\n    # Get a batch\n    images, masks = next(iter(dataloader))\n\n    # Move to CPU for visualization\n    images = images.cpu()\n    masks = masks.cpu()\n\n    # Only display up to the requested number of samples\n    num_samples = min(num_samples, len(images))\n    \n    rows = 4 if index else 2\n    fig, axes = plt.subplots(rows, num_samples, figsize=(5*num_samples, 5*rows))\n    \n    idx = 0\n    while(idx < num_samples):\n        i = np.random.randint(0, len(images))\n        if masks[i].sum() == 0:\n            continue\n\n        # Display RGB channels (assuming bands 3,2,1 are RGB)\n        rgb_img = images[i][[3,2,1]].permute(1,2,0)\n        # Normalize for visualization\n        rgb_img = (rgb_img - rgb_img.min()) / (rgb_img.max() - rgb_img.min())\n        \n        axes[0, idx].imshow(rgb_img)\n        axes[0, idx].axis('off')\n        axes[0, idx].set_title(f'Image {idx+1}')\n        \n        axes[1, idx].imshow(masks[i][0], cmap='gray')\n        axes[1, idx].axis('off')\n        axes[1, idx].set_title(f'Mask {idx+1}')\n\n        if index:\n            axes[2, idx].imshow(images[i][12], cmap='gray')\n            axes[2, idx].axis('off')\n            axes[2, idx].set_title(f'NDWI1 {idx+1}')\n\n            axes[3, idx].imshow(images[i][13], cmap='gray')\n            axes[3, idx].axis('off')\n            axes[3, idx].set_title(f'NDWI2 {idx+1}')\n\n        idx += 1\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-12-14T14:29:53.510872Z","iopub.execute_input":"2024-12-14T14:29:53.511190Z","iopub.status.idle":"2024-12-14T14:29:53.524383Z","shell.execute_reply.started":"2024-12-14T14:29:53.511150Z","shell.execute_reply":"2024-12-14T14:29:53.523646Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"'''VISUALIZE SAMPLES'''\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# device = 'cpu'\nprint(f\"Using device: {device}\")\n\nroot_dir = \"/kaggle/input/sentinel-2-water-edges-dataset/SWED\"\ntrain_loader, val_loader, test_loader = get_dataloaders(root_dir, batch_size=16, num_workers=4)\n\n# Display 5 samples from training set\n# display_samples(train_loader, num_samples=5)\n# display_samples(test_loader, num_samples=5)\n\n'''\n# in case we need standardization\n\nchannel-wise mean:  tensor([ 532.5187,  636.4246,  892.5240, 1049.9366, 1307.1577, 1738.9155,\n        1915.7476, 1995.0083, 2055.7939, 2086.2705, 2001.6875, 1491.3577])\nchannel-wise std:  tensor([ 679.3956,  750.0253,  923.6580, 1273.5732, 1366.0400, 1500.5621,\n        1623.3806, 1687.1169, 1720.2144, 1827.5625, 1932.8875, 1631.7715])\n'''","metadata":{"execution":{"iopub.status.busy":"2024-12-14T14:29:53.525215Z","iopub.execute_input":"2024-12-14T14:29:53.525444Z","iopub.status.idle":"2024-12-14T14:29:54.494553Z","shell.execute_reply.started":"2024-12-14T14:29:53.525421Z","shell.execute_reply":"2024-12-14T14:29:54.493446Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Using device: cuda\nTrain size: 22579\nValidation size: 5645\nTest size: 98\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'\\n# in case we need standardization\\n\\nchannel-wise mean:  tensor([ 532.5187,  636.4246,  892.5240, 1049.9366, 1307.1577, 1738.9155,\\n        1915.7476, 1995.0083, 2055.7939, 2086.2705, 2001.6875, 1491.3577])\\nchannel-wise std:  tensor([ 679.3956,  750.0253,  923.6580, 1273.5732, 1366.0400, 1500.5621,\\n        1623.3806, 1687.1169, 1720.2144, 1827.5625, 1932.8875, 1631.7715])\\n'"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"!pip install -q --upgrade torchmetrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T14:29:54.495991Z","iopub.execute_input":"2024-12-14T14:29:54.496376Z","iopub.status.idle":"2024-12-14T14:30:03.862609Z","shell.execute_reply.started":"2024-12-14T14:29:54.496334Z","shell.execute_reply":"2024-12-14T14:30:03.861552Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"'''TRAINER CLASS'''\nfrom tqdm import tqdm\nfrom torchmetrics.classification import (\n    BinaryAccuracy, BinaryPrecision, BinaryRecall, BinaryF1Score, BinaryJaccardIndex, \n    BinaryCohenKappa, BinaryMatthewsCorrCoef, Accuracy\n)\nimport pandas as pd\n\nclass Trainer:\n    def __init__(self, model, criterion, optimizer, train_loader, val_loader, test_loader, device, \n                 scheduler=None, early_stopping_patience=8, min_delta=0.001):\n        self.model = model\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.test_loader = test_loader\n        self.device = device\n        self.scheduler = scheduler\n        self.predictions = None\n\n        # Early stopping parameters\n        self.early_stopping_patience = early_stopping_patience\n        self.min_delta = min_delta\n        self.best_val_loss = float('inf')\n        self.early_stopping_counter = 0\n        self.early_stopped = False\n\n    def save_checkpoint(self, epoch, train_loss, val_loss, best_model=False):\n        checkpoint = {\n            'epoch': epoch,\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'train_loss': train_loss,\n            'val_loss': val_loss,\n            'train_losses': self.train_losses,\n            'val_losses': self.val_losses\n        }\n        if self.scheduler:\n            checkpoint['scheduler_state_dict'] = self.scheduler.state_dict()\n\n        if best_model:\n            save_path = 'best_model.pt'\n        else:\n            save_path = 'checkpoint.pt'\n        torch.save(checkpoint, save_path)\n    \n    def load_checkpoint(self, checkpoint_path=\"best_model.pt\"):\n        if not os.path.exists(checkpoint_path):\n            return 0  # Start from scratch if no checkpoint exists\n            \n        checkpoint = torch.load(checkpoint_path)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        if self.scheduler and 'scheduler_state_dict' in checkpoint:\n            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        \n        # Restore loss history\n        self.train_losses = checkpoint.get('train_losses', [])\n        self.val_losses = checkpoint.get('val_losses', [])\n        \n        return checkpoint['epoch']\n\n    def train_epoch(self):\n        self.model.train()\n        running_loss = 0.0\n        for images, labels in tqdm(self.train_loader, desc=\"train\"):\n            images, labels = images.to(self.device), labels.to(self.device).float()\n            \n            self.optimizer.zero_grad()\n            outputs = self.model(images)\n            loss = self.criterion(outputs.squeeze(), labels.squeeze())\n            loss.backward()\n            self.optimizer.step()\n            \n            running_loss += loss.item() * images.size(0)\n        \n        return running_loss / len(self.train_loader.dataset)\n\n    def val_epoch(self):\n        self.model.eval()\n        running_loss = 0.0\n        with torch.no_grad():\n            for images, labels in tqdm(self.val_loader, desc=\"validation\"):\n                images, labels = images.to(self.device), labels.to(self.device).float()\n                outputs = self.model(images)\n                loss = self.criterion(outputs.squeeze(), labels.squeeze())\n                running_loss += loss.item() * images.size(0)\n        \n        return running_loss / len(self.val_loader.dataset)\n    \n    def plot_losses(self, train_losses, val_losses):\n        plt.figure(figsize=(10, 5))\n        plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n        plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n        plt.xlabel('Epochs')\n        plt.ylabel('Loss')\n        plt.title('Training and Validation Loss')\n        plt.legend()\n        plt.show()\n    \n    def train(self, num_epochs, do_plot=True, plot_interval=2, resume=True):\n        # Initialize or restore from checkpoint\n        self.train_losses = []\n        self.val_losses = []\n        start_epoch = self.load_checkpoint('checkpoint.pt') if resume else 0\n        self.best_val_loss = float('inf')\n        self.early_stopping_counter = 0\n        self.early_stopped = False\n\n        if start_epoch > 0:\n            print(\"Training resumed from epoch \", start_epoch)\n    \n        for epoch in range(start_epoch, num_epochs):\n            if self.early_stopped:\n                print(\"Early stopping triggered.\")\n                break\n\n            train_loss = self.train_epoch()\n            val_loss = self.val_epoch()\n    \n            self.train_losses.append(train_loss)\n            self.val_losses.append(val_loss)\n            \n            if self.scheduler:\n                self.scheduler.step(val_loss)\n    \n            print(f\"Epoch {epoch + 1}/{num_epochs}\")\n            print(f\"Training Loss: {train_loss:.4f}\")\n            print(f\"Validation Loss: {val_loss:.4f}\")\n\n            self.save_checkpoint(epoch + 1, train_loss, val_loss)\n    \n            # Early stopping logic\n            if val_loss < self.best_val_loss - self.min_delta:\n                self.best_val_loss = val_loss\n                self.early_stopping_counter = 0\n                # Save the best model\n                self.save_checkpoint(epoch + 1, train_loss, val_loss, best_model=True)\n                print(f\"New best model saved at epoch {epoch + 1}\")\n            else:\n                self.early_stopping_counter += 1\n                print(f\"No improvement. Early stopping counter: {self.early_stopping_counter}\")\n                \n                if self.early_stopping_counter >= self.early_stopping_patience:\n                    self.early_stopped = True\n                    print(\"Early stopping triggered.\")\n    \n            if do_plot and (epoch % plot_interval == 0 or epoch == num_epochs - 1):\n                self.plot_losses(self.train_losses, self.val_losses)\n        \n        # Load the best model at the end of training\n        if os.path.exists('best_model.pt'):\n            self.load_checkpoint('best_model.pt')\n        \n        return self.early_stopped\n\n\n    def test(self, thres=0.5):\n        self.load_checkpoint('best_model.pt')\n        self.model.eval()\n        running_loss = 0.0\n        all_predictions = []\n        y_true = None\n        y_pred = None\n\n        with torch.no_grad():\n            for images, labels in tqdm(self.test_loader, desc=\"Testing\"):\n                images, labels = images.to(self.device), labels.to(self.device).float()\n                outputs = self.model(images)\n\n                if y_true is None:\n                    y_true = labels.squeeze().cpu().numpy()\n                    y_pred = (outputs.squeeze().cpu().numpy() > thres).astype(int)\n                else:\n                    y_true = np.concatenate((y_true, labels.squeeze().cpu().numpy()))\n                    y_pred = np.concatenate((y_pred, (outputs.squeeze().cpu().numpy() > 0.5).astype(int)))\n                \n                loss = self.criterion(outputs.squeeze(), labels.squeeze())\n                running_loss += loss.item() * images.size(0)\n                all_predictions.extend(outputs.squeeze().cpu().numpy())\n\n        metrics = self.evaluate_torchmetrics(y_pred, y_true)\n        metrics.loc[len(metrics)] = ['loss', running_loss / len(self.test_loader.dataset)]  \n\n        return metrics\n\n    def evaluate_torchmetrics(self, y_pred, y_true):\n        metrics = {\n            \"accuracy\": BinaryAccuracy(),\n            \"bal_accuracy\": Accuracy(num_classes=2, task=\"multiclass\", average=\"macro\"),\n            \"precision\": BinaryPrecision(),\n            \"recall\": BinaryRecall(),\n            \"f1_score\": BinaryF1Score(),\n            \"jaccard_index\": BinaryJaccardIndex(),\n            \"cohen_kappa\": BinaryCohenKappa(),\n            \"mcc\": BinaryMatthewsCorrCoef()\n        }\n    \n        y_pred = torch.tensor(y_pred).float()\n        y_true = torch.tensor(y_true).float()\n    \n        # result dataframe\n        results = pd.DataFrame(columns=[\"Metric\", \"Value\"])\n        for metric_name, metric in metrics.items():\n            metric_value = metric(y_pred, y_true)\n            results.loc[len(results)] = [metric_name, metric_value.item()]  \n            \n        return results\n\n    def test_visualize(self, n_samples=5, thres=0.5): \n        self.model.eval()\n        y_true = None\n        y_pred = None\n\n        with torch.no_grad():\n            for images, labels in tqdm(self.test_loader, desc=\"Testing\"):\n                images, labels = images.to(self.device), labels.to(self.device).float()\n                outputs = self.model(images)\n\n                if y_true is None:\n                    y_true = labels.squeeze().cpu().numpy()\n                    y_pred = (outputs.squeeze().cpu().numpy() >thres).astype(int)\n                else:\n                    y_true = np.concatenate((y_true, labels.squeeze().cpu().numpy()))\n                    y_pred = np.concatenate((y_pred, (outputs.squeeze().cpu().numpy() > 0.5).astype(int)))\n                \n        for _ in range(n_samples):\n            idx = random.randint(0, len(y_true))\n            fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n            axes[0].imshow(y_true[idx].reshape(256, 256), cmap='gray')\n            axes[0].set_title('True')\n            axes[1].imshow(y_pred[idx].reshape(256, 256), cmap='gray')\n            axes[1].set_title('Predicted')\n            plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-12-14T14:30:03.864175Z","iopub.execute_input":"2024-12-14T14:30:03.864505Z","iopub.status.idle":"2024-12-14T14:30:06.023882Z","shell.execute_reply.started":"2024-12-14T14:30:03.864472Z","shell.execute_reply":"2024-12-14T14:30:06.023184Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"channels = 14","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T14:30:06.024980Z","iopub.execute_input":"2024-12-14T14:30:06.025486Z","iopub.status.idle":"2024-12-14T14:30:06.029300Z","shell.execute_reply.started":"2024-12-14T14:30:06.025443Z","shell.execute_reply":"2024-12-14T14:30:06.028392Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import torch\n\n\ndef Conv2dSame(in_channels, out_channels, kernel_size, use_bias=True, padding_layer=torch.nn.ReflectionPad2d):\n    ka = kernel_size // 2\n    kb = ka - 1 if kernel_size % 2 == 0 else ka\n    return [\n        padding_layer((ka, kb, ka, kb)),\n        torch.nn.Conv2d(in_channels, out_channels, kernel_size, bias=use_bias)\n    ]\n\n\ndef conv2d_bn(in_channels, filters, kernel_size, padding='same', activation='relu'):\n    assert padding == 'same'\n    affine = False if activation == 'relu' or activation == 'sigmoid' else True\n    sequence = []\n    sequence += Conv2dSame(in_channels, filters, kernel_size, use_bias=False)\n    sequence += [torch.nn.BatchNorm2d(filters, affine=affine)]\n    if activation == \"relu\":\n        sequence += [torch.nn.ReLU()]\n    elif activation == \"sigmoid\":\n        sequence += [torch.nn.Sigmoid()]\n    elif activation == 'tanh':\n        sequence += [torch.nn.Tanh()]\n    return torch.nn.Sequential(*sequence)\n\n\nclass MultiResBlock(torch.nn.Module):\n    def __init__(self, in_channels, u, alpha=1.67, use_dropout=False):\n        super().__init__()\n        w = alpha * u\n        self.out_channel = int(w * 0.167) + int(w * 0.333) + int(w * 0.5)\n        self.conv2d_bn = conv2d_bn(in_channels, self.out_channel, 1, activation=None)\n        self.conv3x3 = conv2d_bn(in_channels, int(w * 0.167), 3, activation='relu')\n        self.conv5x5 = conv2d_bn(int(w * 0.167), int(w * 0.333), 3, activation='relu')\n        self.conv7x7 = conv2d_bn(int(w * 0.333), int(w * 0.5), 3, activation='relu')\n        self.bn_1 = torch.nn.BatchNorm2d(self.out_channel)\n        self.relu = torch.nn.ReLU()\n        self.bn_2 = torch.nn.BatchNorm2d(self.out_channel)\n        self.use_dropout = use_dropout\n        if use_dropout:\n            self.dropout = torch.nn.Dropout(0.5)\n\n    def forward(self, inp):\n        if self.use_dropout:\n            x = self.dropout(inp)\n        else:\n            x = inp\n        shortcut = self.conv2d_bn(x)\n        conv3x3 = self.conv3x3(x)\n        conv5x5 = self.conv5x5(conv3x3)\n        conv7x7 = self.conv7x7(conv5x5)\n        out = torch.cat([conv3x3, conv5x5, conv7x7], dim=1)\n        out = self.bn_1(out)\n        out = torch.add(shortcut, out)\n        out = self.relu(out)\n        out = self.bn_2(out)\n        return out\n\n\nclass ResPathBlock(torch.nn.Module):\n    def __init__(self, in_channels, filters):\n        super(ResPathBlock, self).__init__()\n        self.conv2d_bn1 = conv2d_bn(in_channels, filters, 1, activation=None)\n        self.conv2d_bn2 = conv2d_bn(in_channels, filters, 3, activation='relu')\n        self.relu = torch.nn.ReLU()\n        self.bn = torch.nn.BatchNorm2d(filters)\n\n    def forward(self, inp):\n        shortcut = self.conv2d_bn1(inp)\n        out = self.conv2d_bn2(inp)\n        out = torch.add(shortcut, out)\n        out = self.relu(out)\n        out = self.bn(out)\n        return out\n\n\nclass ResPath(torch.nn.Module):\n    def __init__(self, in_channels, filters, length):\n        super(ResPath, self).__init__()\n        self.first_block = ResPathBlock(in_channels, filters)\n        self.blocks = torch.nn.Sequential(*[ResPathBlock(filters, filters) for i in range(length - 1)])\n\n    def forward(self, inp):\n        out = self.first_block(inp)\n        out = self.blocks(out)\n        return out\n\n\nclass ChannelAttentionBlock(nn.Module):\n    def __init__(self, in_channels, reduction=16):\n        super(ChannelAttentionBlock, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(in_channels, in_channels // reduction),\n            nn.GELU(),\n            nn.Linear(in_channels // reduction, in_channels)\n        )\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = self.fc(self.avg_pool(x).squeeze())\n        max_out = self.fc(self.max_pool(x).squeeze())\n        out = avg_out + max_out\n        return self.sigmoid(out).unsqueeze(2).unsqueeze(3)\n\n\nclass SpatialAttentionBlock(nn.Module):\n    def __init__(self, kernel_size=7):\n        super(SpatialAttentionBlock, self).__init__()\n        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=kernel_size // 2)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        out = torch.cat([avg_out, max_out], dim=1)\n        out = self.conv(out)\n        return self.sigmoid(out)\n\n\nclass CBAMBlock(nn.Module):\n    def __init__(self, in_channels, reduction=16, kernel_size=7):\n        super(CBAMBlock, self).__init__()\n        self.channel_attention = ChannelAttentionBlock(in_channels, reduction)\n        self.spatial_attention = SpatialAttentionBlock(kernel_size)\n\n    def forward(self, x):\n        x = x * self.channel_attention(x)\n        x = x * self.spatial_attention(x)\n        return x\n\n\nclass MultiResUnet(torch.nn.Module):\n    def __init__(self, in_channels, out_channels, nf=32, use_dropout=False):\n        super(MultiResUnet, self).__init__()\n        self.mres_block1 = MultiResBlock(in_channels, u=nf)\n        self.pool = torch.nn.MaxPool2d(kernel_size=2)\n        self.res_path1 = ResPath(self.mres_block1.out_channel, nf, 4)\n\n        self.mres_block2 = MultiResBlock(self.mres_block1.out_channel, u=nf * 2)\n        # self.pool2 = torch.nn.MaxPool2d(kernel_size=2)\n        self.res_path2 = ResPath(self.mres_block2.out_channel, nf * 2, 3)\n\n        self.mres_block3 = MultiResBlock(self.mres_block2.out_channel, u=nf * 4)\n        # self.pool3 = torch.nn.MaxPool2d(kernel_size=2)\n        self.res_path3 = ResPath(self.mres_block3.out_channel, nf * 4, 2)\n\n        self.mres_block4 = MultiResBlock(self.mres_block3.out_channel, u=nf * 8)\n        # self.pool4 = torch.nn.MaxPool2d(kernel_size=2)\n        self.res_path4 = ResPath(self.mres_block4.out_channel, nf * 8, 1)\n\n        self.mres_block5 = MultiResBlock(self.mres_block4.out_channel, u=nf * 16)\n        self.cbam_mres_block5 = CBAMBlock(self.mres_block5.out_channel)\n\n        self.deconv1 = torch.nn.ConvTranspose2d(self.mres_block5.out_channel, nf * 8, (2, 2), (2, 2))\n        self.mres_block6 = MultiResBlock(nf * 8 + nf * 8, u=nf * 8, use_dropout=use_dropout)\n        # MultiResBlock(nf * 8 + self.mres_block4.out_channel, u=nf * 8)\n\n        self.deconv2 = torch.nn.ConvTranspose2d(self.mres_block6.out_channel, nf * 4, (2, 2), (2, 2))\n        self.mres_block7 = MultiResBlock(nf * 4 + nf * 4, u=nf * 4, use_dropout=use_dropout)\n        # MultiResBlock(nf * 4 + self.mres_block3.out_channel, u=nf * 4)\n\n        self.deconv3 = torch.nn.ConvTranspose2d(self.mres_block7.out_channel, nf * 2, (2, 2), (2, 2))\n        self.mres_block8 = MultiResBlock(nf * 2 + nf * 2, u=nf * 2, use_dropout=use_dropout)\n        # MultiResBlock(nf * 2 + self.mres_block2.out_channel, u=nf * 2)\n\n        self.deconv4 = torch.nn.ConvTranspose2d(self.mres_block8.out_channel, nf, (2, 2), (2, 2))\n        self.mres_block9 = MultiResBlock(nf + nf, u=nf)\n        # MultiResBlock(nf + self.mres_block1.out_channel, u=nf)\n\n        \n        self.cbam_mres_block9 = CBAMBlock(self.mres_block9.out_channel)\n\n        self.conv10 = conv2d_bn(self.mres_block9.out_channel, out_channels, 1, padding='same', activation='tanh')\n\n    def forward(self, inp):\n        mresblock1 = self.mres_block1(inp)\n        pool = self.pool(mresblock1)\n        mresblock1 = self.res_path1(mresblock1)\n\n        mresblock2 = self.mres_block2(pool)\n        pool = self.pool(mresblock2)\n        mresblock2 = self.res_path2(mresblock2)\n\n        mresblock3 = self.mres_block3(pool)\n        pool = self.pool(mresblock3)\n        mresblock3 = self.res_path3(mresblock3)\n\n        mresblock4 = self.mres_block4(pool)\n        pool = self.pool(mresblock4)\n        mresblock4 = self.res_path4(mresblock4)\n\n        mresblock = self.mres_block5(pool)\n        mresblock = self.cbam_mres_block5(mresblock)\n\n        up = torch.cat([self.deconv1(mresblock), mresblock4], dim=1)\n        mresblock = self.mres_block6(up)\n\n        up = torch.cat([self.deconv2(mresblock), mresblock3], dim=1)\n        mresblock = self.mres_block7(up)\n\n        up = torch.cat([self.deconv3(mresblock), mresblock2], dim=1)\n        mresblock = self.mres_block8(up)\n\n        up = torch.cat([self.deconv4(mresblock), mresblock1], dim=1)\n        mresblock = self.mres_block9(up)\n\n        mresblock = self.cbam_mres_block9(mresblock)\n        conv10 = self.conv10(mresblock)\n        return conv10\n\n\nclass MultiResUnetGenerator(torch.nn.Module):\n    def __init__(self, input_nc, output_nc, ngf=64, use_dropout=False, gpu_ids=[]):\n        super(MultiResUnetGenerator, self).__init__()\n        self.gpu_ids = gpu_ids\n\n        self.model = MultiResUnet(input_nc, output_nc, nf=ngf, use_dropout=use_dropout)\n\n    def forward(self, inp):\n        if self.gpu_ids and isinstance(inp.data, torch.cuda.FloatTensor):\n            return torch.nn.parallel.data_parallel(self.model, inp, self.gpu_ids)\n        else:\n            return self.model(inp)\n\n\n# a = ResPath(10, 100,3)\n# a.apply(weights_init_uniform_rule)\nmodel = MultiResUnet(14, 1, 32, False).to(device)\nx = torch.randn(4, 14, 128, 128).to(device)\nprint(model(x).shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T14:30:06.030733Z","iopub.execute_input":"2024-12-14T14:30:06.031216Z","iopub.status.idle":"2024-12-14T14:30:06.757483Z","shell.execute_reply.started":"2024-12-14T14:30:06.031174Z","shell.execute_reply":"2024-12-14T14:30:06.756432Z"}},"outputs":[{"name":"stdout","text":"torch.Size([4, 1, 128, 128])\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"param_size = 0\nfor param in model.parameters():\n    param_size += param.nelement() * param.element_size()\nbuffer_size = 0\nfor buffer in model.buffers():\n    buffer_size += buffer.nelement() * buffer.element_size()\n\nsize_all_mb = (param_size + buffer_size) / 1024**2\nprint('model size: {:.3f}MB'.format(size_all_mb))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T14:30:06.758854Z","iopub.execute_input":"2024-12-14T14:30:06.759189Z","iopub.status.idle":"2024-12-14T14:30:06.766368Z","shell.execute_reply.started":"2024-12-14T14:30:06.759153Z","shell.execute_reply":"2024-12-14T14:30:06.765568Z"}},"outputs":[{"name":"stdout","text":"model size: 28.062MB\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"class FocalLoss(nn.Module):\n    def __init__(self, alpha=0.6, gamma=2.0):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.bce_loss = nn.BCEWithLogitsLoss(reduction=\"none\")\n    \n    def forward(self, pred, target):\n        pred, target = pred.squeeze(), target.squeeze()\n        loss = self.bce_loss(pred, target)\n        prob = torch.sigmoid(pred)  # Predicted probability\n        alpha = torch.where(target == 1, self.alpha, 1 - self.alpha)  # Class balancing factor\n        focal_weight = torch.where(target == 1, 1 - prob, prob)  # Focusing weight\n        focal_weight = alpha * focal_weight**self.gamma  # Apply alpha and gamma\n        focal_loss = focal_weight * loss\n        \n        focal_loss = focal_loss.sum(dim=(-2,-1)) #*mask\n        return focal_loss.mean()\n\ndef loss(pred, target):\n#     print(pred.shape, target.shape)\n    bce_loss = nn.BCEWithLogitsLoss(reduction=\"none\") \n    ll = bce_loss(pred, target)\n\n    ll = ll.sum(dim=(-2,-1)) #*mask\n    return ll.mean()\n\n\npred = torch.randn(16, 128, 128).float()\ntarget = torch.randint(0, 2, (16, 128, 128)).float()\n\n# focal_loss = FocalLoss(alpha=0.75, gamma=2.0)\n# focal_loss(pred, target)\nloss(pred, target)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T14:30:06.767535Z","iopub.execute_input":"2024-12-14T14:30:06.768279Z","iopub.status.idle":"2024-12-14T14:30:06.796788Z","shell.execute_reply.started":"2024-12-14T14:30:06.768248Z","shell.execute_reply":"2024-12-14T14:30:06.795997Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"tensor(13197.2324)"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"import torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport pandas as pd\n\ncriterion = FocalLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=4, verbose=True)\n\ntrainer = Trainer(model, criterion, optimizer, train_loader, val_loader, test_loader, device, scheduler)\ntrainer.train(num_epochs=30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T14:30:06.797882Z","iopub.execute_input":"2024-12-14T14:30:06.798426Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_1556/740989651.py:52: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(checkpoint_path)\n","output_type":"stream"},{"name":"stdout","text":"Training resumed from epoch  1\n","output_type":"stream"},{"name":"stderr","text":"train:  42%|████▏     | 589/1412 [06:12<08:37,  1.59it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"metrics_df = trainer.test(thres=0.5)\nmetrics_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.test_visualize(10, 0.5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !rm -r /kaggle/working/*","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}